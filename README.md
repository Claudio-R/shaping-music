Creative Programming and Computing
A. Y. 2022-2023
Abstract of the project

# Shaping Music

## Group 6 - Cagnolino

## Members of the group 
1.	[Claudio Rapisarda] [https://github.com/Claudio-R] 
2.	[Olga Besedova] [https://github.com/Oliffka]
3.	[Silvia Messana] [https://github.com/SissiPrinciSexy]
4.	[Alessandro Zullo] [https://github.com/Alessandro199762]

## Github repository
[https://github.com/Claudio-R/ShapingMusic]


# Abstract
Shaping Music is a project with which we aim at developing a user friendly app whose will is to introduce the point of view of a machine in a creative process.

The core idea is to let the machine generate a musical video based on a song provided by the user.

The final output of the project will be an app that exploits two main techonolgies: Autoencoders and Conditional Generative Adversarial Net (GANs).

The designing process will go through mainly two steps.

As far as it concerns the training step, the autoencoder neural netwoek will be used for linking together the song's semantic conent and the associated video taken from the dataset.

It creates a compressed representation of the original input (both the song and the video) that will allow the computation of an associative function that, once applied to the dataset, links some fetures of the video to some properties of its song.

Once this step is complete, the testing step will start, during which the Conditional GAN is used.

This technology will take as input the song selected by the user and a video static noise, chosen because of its intrinsic randomicity, and will provide an autogenerated generated video as the output.

The previously generated associative function is then applied at both the song and the newly generated video which is then fed to the discrimantor that will try to understand whether its input is a real video or an autogenerated one.

Finally, the discriminator's output is fed into the generator to improve its properties and the final output.

## Artistic vision
Despite the very complex names of the pieces of technology that this software would exploit, this app is meant to be used by everyone.

Infact, its versatility is such as to be used both by amateurs or a professional video-makers.

While on one hand, indeed, it could be used for entertainment purpouses, on the other it can be exploited to help a video-maker see things that the human eye can't.

With the advance of deep learning and neural network, indeed, it is now possible to generate an audio-visual content whose ideas can't be contained inside the human's mind only.

Curious and funny associations might arise when dealing with such technologies, and it would be a shame not to exploit this artistic drive that seems to naturally arise by this new frontier of wire-made artists.
Moreover, this cold be that very project that succed in demonstrating that machines can touch emotions just like humans do.

## Prototype after the hackathon
The first step in implementing the prototype is preprocessing the dataset, which assumes to split the music video into audio and video contents. The next step is to extract the proper features from both, audio and video content, and use these features as input for autoencoder. As an output we expect to get multidimentional space representing the relation between audio and video content.

 1. Finding a proper dataset with music videos and filtering it (if needed);
 2. Dataset preprocessing

    2.1 Extract features  from audio and find a proper representation;
    
    2.2 Extract features  from video and find a proper representation;
 3. Implement an autoencoder to make it easier to relate audio and video content;

    3.1 Design the model
    
    3.2 Feed the autoencoder on both, audio and video representations;
    
    3.3 Visualize the latent space.

# Final project
The final goal of our project is to create a system which is able to generate video content for an input song. To do so, we are going to use the output of the autoencoder (which will be presented as a prototype), as the ground truth, to train a GAN, which is responsible for generating a video content matching the song (in terms of semantic and emotional content).

 1. Finding a proper dataset with music videos and probably filtering it if needed;
 2. Dataset preprocessing which implies extracting features  from both, audio and video contents;
 3. Implement an autoencoder to make it easier to relate audio and video content;
 4. Implement a GAN in the following way:

    4.1 The generator will take as input a song (or better, a segment of the song) and will output a sequence of frames;

    4.2 Use the same autoencoder as before to find the relation between the input of the generator and the generated sequence of frames;

    4.3 The discriminator should distinguish the output of the autoencoder (from 4.2) from the ground truth; 

    4.4 Use the output of the discriminator to update the GAN.


# How to use it
From the root folder run the following command to install the dependencies:

`pip install -r ./docs/requirements.txt`

Then call:

`python3 app.py`