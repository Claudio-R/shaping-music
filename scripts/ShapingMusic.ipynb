{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xlTv6JKLfJ6"
      },
      "source": [
        "# Project Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYF8JjaBLaUo"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import glob\n",
        "import librosa\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython import display\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "import moviepy.video.io.ImageSequenceClip\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "from . import yamnet\n",
        "from . import utils\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD YAMNET PACKAGE TO THE SYSTEM PATHS\n",
        "import sys\n",
        "absolute_path_yamnet = '/home/claudio-unix/mae/ShapingMusic/yamnet_pkg'\n",
        "sys.path.insert(1, absolute_path_yamnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMAGES_FEATURES_EXTRACTOR_MODEL\n",
        "image_layers = [\n",
        "    'block1_conv1',\n",
        "    'block2_conv1',\n",
        "    'block3_conv1', \n",
        "    'block4_conv1', \n",
        "    'block5_conv1'\n",
        "    ]\n",
        "\n",
        "image_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "image_model.trainable = False\n",
        "outputs = [image_model.get_layer(name).output for name in image_layers]\n",
        "image_FEM = tf.keras.Model(image_model.input, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUDIO MODEL\n",
        "audio_layers = [\n",
        "    'layer1/pointwise_conv',\n",
        "    'layer2/pointwise_conv',\n",
        "    'layer3/pointwise_conv',\n",
        "    'layer4/pointwise_conv',\n",
        "    'layer5/pointwise_conv',\n",
        "    'layer6/pointwise_conv',\n",
        "    'layer7/pointwise_conv',\n",
        "    'layer8/pointwise_conv',\n",
        "    'layer9/pointwise_conv',\n",
        "    'layer10/pointwise_conv',\n",
        "    'layer12/pointwise_conv',\n",
        "    'layer13/pointwise_conv',\n",
        "    'layer14/pointwise_conv',\n",
        "    ]\n",
        "\n",
        "audio_model = yamnet.inference.YamNet()\n",
        "audio_model.trainable = False\n",
        "outputs = [audio_model.get_layer(name).output for name in audio_layers]\n",
        "audio_FEM = tf.keras.Model(audio_model.input, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEXT-TO-IMAGE GENERATOR\n",
        "import yaml\n",
        "import openai\n",
        "with open(\"apikey.local.yml\", \"r\") as stream:\n",
        "    try:\n",
        "        openai.api_key = yaml.safe_load(stream)['apikey']\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "        print('Cannot load the APIKey')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import videotest\n",
        "\n",
        "class Audio_Image_Classifier:\n",
        "    '''\n",
        "    Class responsible for multimodal classification.\n",
        "    Given an audio track and the corresponding sequence of frames to classify, produces a string by concatenating labels.\n",
        "    sound_model: pretrained tf.Model for audio classification\n",
        "    image_model: pretrained tf.Model for image classification\n",
        "    '''\n",
        "\n",
        "    def __init__(self, image_model, sound_model):\n",
        "        self.image_model = image_model\n",
        "        self.sound_model = sound_model\n",
        "        self.frame_count = 0\n",
        "        self.fps = 2\n",
        "        self.frames_path = 'data/test_frames'\n",
        "        self.sounds_path = 'data/test_samples/'\n",
        "        self.full_audios_path = 'data/test_audio'\n",
        "        self.generated_frames_path = 'data/generated_frames/'\n",
        "        self.generated_clips_path = 'data/generated_clips/'\n",
        "\n",
        "\n",
        "    def __call__(self, video_url):\n",
        "        \n",
        "        if(video_url.endswith('.mp4') == False):\n",
        "            return\n",
        "        self.filename = video_url[0:-4]\n",
        "\n",
        "        self.frame_count = 0\n",
        "        self.__preprocess_video(video_url)\n",
        "\n",
        "        img_list, snd_list = self.__retrieve_data()\n",
        "        \n",
        "        for i in range(len(img_list)):\n",
        "            self.__classify(img_list[i], snd_list[i])\n",
        "        print('Completed! Generating clip...')\n",
        "        \n",
        "        self.__generate_clip()\n",
        "        \n",
        "        self.__cleanup()\n",
        "        \n",
        "        print('DONE!')\n",
        "\n",
        "\n",
        "    def __preprocess_video(self, video_url):\n",
        "        videotest.main(video_url)\n",
        "    \n",
        "    \n",
        "    def __retrieve_data(self):\n",
        "        frames_list = []\n",
        "        for filename in os.listdir(self.frames_path):\n",
        "            path = os.path.join(self.frames_path, filename)\n",
        "            frames_list.append(path)\n",
        "\n",
        "        sounds_list = []\n",
        "        for filename in os.listdir(self.sounds_path):\n",
        "            path = os.path.join(self.sounds_path, filename)\n",
        "            frames_list.append(path)\n",
        "            \n",
        "        return (frames_list, sounds_list)\n",
        "\n",
        "\n",
        "    def __classify(self, img, snd):\n",
        "\n",
        "        # Image Classification\n",
        "        img = load_img(img)\n",
        "        input_img = tf.keras.applications.vgg19.preprocess_input(img * 255)\n",
        "        input_img = tf.image.resize(input_img, (224, 224))\n",
        "        predictions_img = self.image_model(input_img)\n",
        "        predictions_img = tf.keras.applications.vgg19.decode_predictions(predictions_img.numpy())[0]\n",
        "\n",
        "        # Sound Classification\n",
        "        predictions_snd = audio_model(snd)\n",
        "\n",
        "        # Clip Generation\n",
        "        text = self.__parse_predictions(predictions_img, predictions_snd)\n",
        "        try:\n",
        "            self.__generate_image(text)\n",
        "        except:\n",
        "            try: \n",
        "                print(text)\n",
        "                print('Your prompt may contain text that is not allowed by our safety system.')\n",
        "                text = predictions_snd\n",
        "                self.__generate_image(text)\n",
        "            except:\n",
        "                try:\n",
        "                    print(text)\n",
        "                    print('Your prompt may contain text that is not allowed by our safety system.')\n",
        "                    text = self.__parse_predictions(predictions_img, [])\n",
        "                    self.__generate_image(text)\n",
        "                except:\n",
        "                    print(text)\n",
        "                    print('Your prompt may contain text that is not allowed by our safety system.')\n",
        "                    self.__generate_image(predictions_snd[0])\n",
        "\n",
        "\n",
        "    def __parse_predictions(self, predictions_img, predictions_snd):\n",
        "        predictions_img = [class_name for (number, class_name, prob) in predictions_img]\n",
        "        predictions_img = [class_name.split('_') for class_name in predictions_img]\n",
        "        input_text = predictions_snd\n",
        "        for words in predictions_img:\n",
        "            for w in words:\n",
        "                input_text.append(w)\n",
        "        return ' '.join(input_text)\n",
        "\n",
        "\n",
        "    def __generate_image(self, text):\n",
        "        response = openai.Image.create(\n",
        "            prompt = text,\n",
        "            n=1,\n",
        "            size=\"512x512\"\n",
        "        )\n",
        "        image_url = response['data'][0]['url']\n",
        "        response = requests.get(image_url)\n",
        "        img = PIL.Image.open(BytesIO(response.content))\n",
        "        self.__store_image(img)\n",
        "\n",
        "\n",
        "    def __store_image(self, img):\n",
        "        generated_frame_path = 'img_at_frame_{}.png'.format(self.frame_count)\n",
        "        path = os.path.join(self.generated_frames_path, generated_frame_path)\n",
        "        tf.keras.utils.save_img(path, img)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def __generate_clip(self):\n",
        "        output_file_path = os.path.join(self.generated_clips_path, 'Test.mp4')\n",
        "        \n",
        "        generated_frames_list = []\n",
        "        for filename in os.listdir(self.images_path):\n",
        "            path = os.path.join(self.images_path, filename)\n",
        "            generated_frames_list.append(path)\n",
        "\n",
        "        clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(generated_frames_list, self.fps)\n",
        "        clip.write_videofile(output_file_path)\n",
        "\n",
        "        audio_clip = AudioFileClip(self.audio_path)\n",
        "        video_clip = VideoFileClip(output_file_path).set_audio(audio_clip)\n",
        "        video_clip.write_videofile(output_file_path)\n",
        "    \n",
        "\n",
        "    def __cleanup(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier = Audio_Image_Classifier(image_FEM, audio_FEM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier('data/test_video/test1.mp4')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd47853e32cf332c7e23a177cc7406abfb96bfc3fe21f7dd172e4dde369451e1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
