{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xlTv6JKLfJ6"
      },
      "source": [
        "# Project Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZYF8JjaBLaUo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-19 00:19:07.631299: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-19 00:19:07.787490: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-01-19 00:19:07.793061: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-01-19 00:19:07.793076: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-01-19 00:19:08.478368: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-19 00:19:08.478486: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-19 00:19:08.478493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'2.11.0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import glob\n",
        "import librosa\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython import display\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "import moviepy.video.io.ImageSequenceClip\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD YAMNET PACKAGE TO THE SYSTEM PATHS\n",
        "import sys\n",
        "absolute_path_yamnet = '/home/claudio-unix/mae/ShapingMusic/yamnet_pkg'\n",
        "sys.path.insert(1, absolute_path_yamnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UTILS\n",
        "def load_img(path_to_img):\n",
        "    max_dim = 512\n",
        "    img = tf.io.read_file(path_to_img)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "    long_dim = max(shape)\n",
        "    scale = max_dim / long_dim\n",
        "\n",
        "    new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "    img = tf.image.resize(img, new_shape)\n",
        "    img = img[tf.newaxis, :]\n",
        "    return img\n",
        "\n",
        "def imshow(image, title=None):\n",
        "    if len(image.shape) > 3:\n",
        "        image = tf.squeeze(image, axis=0)\n",
        "        plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "\n",
        "def split_audio(file_name):\n",
        "    audio, FS = librosa.load(file_name)\n",
        "    Ts = 0.5\n",
        "    hop_size = int(Ts*FS)\n",
        "    N = int(len(audio)/hop_size)\n",
        "    audio_list = [ audio[i*hop_size:(i+1)*hop_size] for i in range(N)]\n",
        "    return audio_list\n",
        "\n",
        "def retrieve_frames(path_dir):\n",
        "    frames_list = []\n",
        "    for filename in os.listdir(path_dir):\n",
        "        path = os.path.join(path_dir, filename)\n",
        "        # path = os.path.join(os.getcwd(), path)\n",
        "        frames_list.append(path)\n",
        "    return frames_list\n",
        "\n",
        "def create_clip(images_path, audio_path):\n",
        "    generated_frames_list = []\n",
        "    for filename in os.listdir(images_path):\n",
        "        path = os.path.join(images_path, filename)\n",
        "        # path = os.path.join(os.getcwd(), path)\n",
        "        generated_frames_list.append(path)\n",
        "\n",
        "    # response = requests.get(image_url)\n",
        "    # image = Image.open(BytesIO(response.content))\n",
        "    # clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(generated_frames_list, fps)\n",
        "    # audio_clip = moviepy.audio.io.AudioFileClip.AudioFileClip(audio_filename)\n",
        "    \n",
        "    # clip.set_audio(audio_clip);\n",
        "    # clip.write_videofile(video_filename)\n",
        "    # np_image = np.array(image)\n",
        "\n",
        "    fps = 2\n",
        "    audio_filename = audio_path\n",
        "    video_filename = 'data/generated_clip/Test.mp4'\n",
        "\n",
        "    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(generated_frames_list, fps)\n",
        "    clip.write_videofile(video_filename)\n",
        "\n",
        "    audio_clip = AudioFileClip(audio_filename)\n",
        "    # audio_clip = audio_clip.subclip(0, 25)\n",
        "\n",
        "    video_clip = VideoFileClip(video_filename);\n",
        "    video_clip = video_clip.set_audio(audio_clip);\n",
        "\n",
        "    video_clip.write_videofile(video_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMAGES_FEATURES_EXTRACTOR_MODEL\n",
        "image_layers = [\n",
        "    'block1_conv1',\n",
        "    'block2_conv1',\n",
        "    'block3_conv1', \n",
        "    'block4_conv1', \n",
        "    'block5_conv1'\n",
        "    ]\n",
        "\n",
        "image_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "image_model.trainable = False\n",
        "outputs = [image_model.get_layer(name).output for name in image_layers]\n",
        "images_FEM = tf.keras.Model(image_model.input, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-19 00:19:15.331861: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/claudio-unix/mae/ShapingMusic/myenv/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2023-01-19 00:19:15.331905: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-01-19 00:19:15.331938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (popcorn): /proc/driver/nvidia/version does not exist\n",
            "2023-01-19 00:19:15.332282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# AUDIO MODEL\n",
        "from yamnet_pkg import inference\n",
        "audio_model = inference.YamNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'output'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m audio_layers \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlayer2/pointwise_conv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m# 'layer3/pointwise_conv',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# 'layer14/pointwise_conv',\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m audio_model\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[39m=\u001b[39m [audio_model\u001b[39m.\u001b[39mget_layer(name)\u001b[39m.\u001b[39moutput \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m audio_layers]\n\u001b[1;32m     17\u001b[0m audios_FEM \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(audio_model\u001b[39m.\u001b[39minput, outputs)\n\u001b[1;32m     18\u001b[0m audios_FEM\u001b[39m.\u001b[39msummary()\n",
            "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m audio_layers \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlayer2/pointwise_conv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m# 'layer3/pointwise_conv',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# 'layer14/pointwise_conv',\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m audio_model\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[39m=\u001b[39m [audio_model\u001b[39m.\u001b[39;49mget_layer(name)\u001b[39m.\u001b[39;49moutput \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m audio_layers]\n\u001b[1;32m     17\u001b[0m audios_FEM \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(audio_model\u001b[39m.\u001b[39minput, outputs)\n\u001b[1;32m     18\u001b[0m audios_FEM\u001b[39m.\u001b[39msummary()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'output'"
          ]
        }
      ],
      "source": [
        "audio_layers = [\n",
        "    'layer1/conv',\n",
        "    'layer2/pointwise_conv',\n",
        "    'layer3/pointwise_conv',\n",
        "    'layer4/pointwise_conv',\n",
        "    'layer5/pointwise_conv',\n",
        "    'layer6/pointwise_conv',\n",
        "    'layer7/pointwise_conv',\n",
        "    'layer8/pointwise_conv',\n",
        "    'layer9/pointwise_conv',\n",
        "    'layer10/pointwise_conv',\n",
        "    'layer12/pointwise_conv',\n",
        "    'layer13/pointwise_conv',\n",
        "    'layer14/pointwise_conv',\n",
        "]\n",
        "audio_model.trainable = False\n",
        "outputs = [audio_model.get_layer(name).output for name in audio_layers]\n",
        "audios_FEM = tf.keras.Model(audio_model.input, outputs)\n",
        "audios_FEM.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEXT-TO-IMAGE GENERATOR\n",
        "import yaml\n",
        "import openai\n",
        "with open(\"apikey.local.yml\", \"r\") as stream:\n",
        "    try:\n",
        "        openai.api_key = yaml.safe_load(stream)['apikey']\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "        print('Cannot load the APIKey')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import videotest\n",
        "\n",
        "class Audio_Image_Classifier:\n",
        "    '''\n",
        "    Class responsible for multimodal classification.\n",
        "    Given an audio track and the corresponding sequence of frames to classify, produces a string by concatenating labels.\n",
        "    sound_model: pretrained tf.Model for audio classification\n",
        "    image_model: pretrained tf.Model for image classification\n",
        "    '''\n",
        "\n",
        "    def __init__(self, image_model, sound_model):\n",
        "        self.image_model = image_model\n",
        "        self.sound_model = sound_model\n",
        "        self.frame_count = 0\n",
        "        self.fps = 2\n",
        "        self.frames_path = 'data/test_frames'\n",
        "        self.sounds_path = 'data/test_samples/'\n",
        "        self.full_audios_path = 'data/test_audio'\n",
        "        self.generated_frames_path = 'data/generated_frames/'\n",
        "        self.generated_clips_path = 'data/generated_clips/'\n",
        "\n",
        "\n",
        "    def __call__(self, video_url):\n",
        "        \n",
        "        if(video_url.endswith('.mp4') == False):\n",
        "            return\n",
        "        self.filename = video_url[0:-4]\n",
        "\n",
        "        self.frame_count = 0\n",
        "        self.__preprocess_video(video_url)\n",
        "\n",
        "        img_list, snd_list = self.__retrieve_data()\n",
        "        \n",
        "        for i in range(len(img_list)):\n",
        "            self.__classify(img_list[i], snd_list[i])\n",
        "        print('Completed! Generating clip...')\n",
        "        \n",
        "        self.__generate_clip()\n",
        "        \n",
        "        self.__cleanup()\n",
        "        \n",
        "        print('DONE!')\n",
        "\n",
        "\n",
        "    def __preprocess_video(self, video_url):\n",
        "        videotest.main(video_url)\n",
        "    \n",
        "    \n",
        "    def __retrieve_data(self):\n",
        "        frames_list = []\n",
        "        for filename in os.listdir(self.frames_path):\n",
        "            path = os.path.join(self.frames_path, filename)\n",
        "            frames_list.append(path)\n",
        "\n",
        "        sounds_list = []\n",
        "        for filename in os.listdir(self.sounds_path):\n",
        "            path = os.path.join(self.sounds_path, filename)\n",
        "            frames_list.append(path)\n",
        "            \n",
        "        return (frames_list, sounds_list)\n",
        "\n",
        "\n",
        "    def __classify(self, img, snd):\n",
        "\n",
        "        # Image Classification\n",
        "        img = load_img(img)\n",
        "        input_img = tf.keras.applications.vgg19.preprocess_input(img * 255)\n",
        "        input_img = tf.image.resize(input_img, (224, 224))\n",
        "        predictions_img = self.image_model(input_img)\n",
        "        predictions_img = tf.keras.applications.vgg19.decode_predictions(predictions_img.numpy())[0]\n",
        "\n",
        "        # Sound Classification\n",
        "        predictions_snd = sound_model(snd)\n",
        "\n",
        "        # Clip Generation\n",
        "        text = self.__parse_predictions(predictions_img, predictions_snd)\n",
        "        try:\n",
        "            self.__generate_image(text)\n",
        "        except:\n",
        "            try: \n",
        "                print(text)\n",
        "                print('Your prompt may contain text that is not allowed by our safety system.')\n",
        "                text = predictions_snd\n",
        "                self.__generate_image(text)\n",
        "            except:\n",
        "                try:\n",
        "                    print(text)\n",
        "                    print('Your prompt may contain text that is not allowed by our safety system.')\n",
        "                    text = self.__parse_predictions(predictions_img, [])\n",
        "                    self.__generate_image(text)\n",
        "                except:\n",
        "                    print(text)\n",
        "                    print('Your prompt may contain text that is not allowed by our safety system.')\n",
        "                    self.__generate_image(predictions_snd[0])\n",
        "\n",
        "\n",
        "    def __parse_predictions(self, predictions_img, predictions_snd):\n",
        "        predictions_img = [class_name for (number, class_name, prob) in predictions_img]\n",
        "        predictions_img = [class_name.split('_') for class_name in predictions_img]\n",
        "        input_text = predictions_snd\n",
        "        for words in predictions_img:\n",
        "            for w in words:\n",
        "                input_text.append(w)\n",
        "        return ' '.join(input_text)\n",
        "\n",
        "\n",
        "    def __generate_image(self, text):\n",
        "        response = openai.Image.create(\n",
        "            prompt = text,\n",
        "            n=1,\n",
        "            size=\"512x512\"\n",
        "        )\n",
        "        image_url = response['data'][0]['url']\n",
        "        response = requests.get(image_url)\n",
        "        img = PIL.Image.open(BytesIO(response.content))\n",
        "        self.__store_image(img)\n",
        "\n",
        "\n",
        "    def __store_image(self, img):\n",
        "        generated_frame_path = 'img_at_frame_{}.png'.format(self.frame_count)\n",
        "        path = os.path.join(self.generated_frames_path, generated_frame_path)\n",
        "        tf.keras.utils.save_img(path, img)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def __generate_clip(self):\n",
        "        output_file_path = os.path.join(self.generated_clips_path, 'Test.mp4')\n",
        "        \n",
        "        generated_frames_list = []\n",
        "        for filename in os.listdir(self.images_path):\n",
        "            path = os.path.join(self.images_path, filename)\n",
        "            generated_frames_list.append(path)\n",
        "\n",
        "        clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(generated_frames_list, self.fps)\n",
        "        clip.write_videofile(output_file_path)\n",
        "\n",
        "        audio_clip = AudioFileClip(self.audio_path)\n",
        "        video_clip = VideoFileClip(output_file_path).set_audio(audio_clip)\n",
        "        video_clip.write_videofile(output_file_path)\n",
        "    \n",
        "\n",
        "    def __cleanup(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier = Audio_Image_Classifier(image_model, sound_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier('data/test_video/test1.mp4')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd47853e32cf332c7e23a177cc7406abfb96bfc3fe21f7dd172e4dde369451e1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
